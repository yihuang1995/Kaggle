{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-cleaning",
   "metadata": {
    "papermill": {
     "duration": 0.015938,
     "end_time": "2023-09-05T00:48:48.852499",
     "exception": false,
     "start_time": "2023-09-05T00:48:48.836561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CommonLit - Evaluate Student Summaries\n",
    "# Introduction\n",
    "Create a quality assessment model for summaries written by students from grade 3 to grade 12. The quality will be evaluated based on the following two criteria:\n",
    "  - content: How well the summary captures the main ideas and details of the source text\n",
    "  - wording: The clarity, precision, and fluency of the language used in the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-machine",
   "metadata": {
    "papermill": {
     "duration": 0.014297,
     "end_time": "2023-09-05T00:48:48.907410",
     "exception": false,
     "start_time": "2023-09-05T00:48:48.893113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Data Loading\n",
    "Loading the data and displaying basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "further-hepatitis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:49:08.145264Z",
     "iopub.status.busy": "2023-09-12T06:49:08.144955Z",
     "iopub.status.idle": "2023-09-12T06:49:12.461403Z",
     "shell.execute_reply": "2023-09-12T06:49:12.460249Z",
     "shell.execute_reply.started": "2023-09-12T06:49:08.145236Z"
    },
    "papermill": {
     "duration": 0.024226,
     "end_time": "2023-09-05T00:48:48.941632",
     "exception": false,
     "start_time": "2023-09-05T00:48:48.917406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "driving-payroll",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:49:12.464103Z",
     "iopub.status.busy": "2023-09-12T06:49:12.463516Z",
     "iopub.status.idle": "2023-09-12T06:49:12.470287Z",
     "shell.execute_reply": "2023-09-12T06:49:12.469172Z",
     "shell.execute_reply.started": "2023-09-12T06:49:12.464057Z"
    },
    "papermill": {
     "duration": 0.016768,
     "end_time": "2023-09-05T00:48:48.967337",
     "exception": false,
     "start_time": "2023-09-05T00:48:48.950569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompts_train_path = \"/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv\"\n",
    "# propmts_test_path = \"/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv\"\n",
    "\n",
    "# summaries_train_path = \"/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv\"\n",
    "# summaries_test_path = \"/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv\"\n",
    "\n",
    "prompts_train_path = \"prompts_train.csv\"\n",
    "propmts_test_path = \"prompts_test.csv\"\n",
    "\n",
    "summaries_train_path = \"summaries_train.csv\"\n",
    "summaries_test_path = \"summaries_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surrounded-battery",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:49:12.473599Z",
     "iopub.status.busy": "2023-09-12T06:49:12.471919Z",
     "iopub.status.idle": "2023-09-12T06:49:12.635652Z",
     "shell.execute_reply": "2023-09-12T06:49:12.634608Z",
     "shell.execute_reply.started": "2023-09-12T06:49:12.473566Z"
    },
    "papermill": {
     "duration": 0.152559,
     "end_time": "2023-09-05T00:48:49.128763",
     "exception": false,
     "start_time": "2023-09-05T00:48:48.976204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load files\n",
    "df_prompts_train = pd.read_csv(prompts_train_path)\n",
    "df_propmts_test = pd.read_csv(propmts_test_path)\n",
    "\n",
    "df_summaries_train = pd.read_csv(summaries_train_path)\n",
    "df_summaries_test = pd.read_csv(summaries_test_path)\n",
    "\n",
    "# merge files\n",
    "df_train = df_summaries_train.merge(df_prompts_train, on=\"prompt_id\")\n",
    "df_test = df_summaries_test.merge(df_propmts_test, on=\"prompt_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italian-currency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:49:12.640596Z",
     "iopub.status.busy": "2023-09-12T06:49:12.639761Z",
     "iopub.status.idle": "2023-09-12T06:49:12.666333Z",
     "shell.execute_reply": "2023-09-12T06:49:12.665380Z",
     "shell.execute_reply.started": "2023-09-12T06:49:12.640569Z"
    },
    "papermill": {
     "duration": 0.029041,
     "end_time": "2023-09-05T00:48:49.167490",
     "exception": false,
     "start_time": "2023-09-05T00:48:49.138449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0095993991fe</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave only started as an experiment w...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "2  0095993991fe    814d6b  The third wave only started as an experiment w...   \n",
       "\n",
       "    content   wording                                    prompt_question  \\\n",
       "0  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "1  3.272894  3.219757  Summarize how the Third Wave developed over su...   \n",
       "2  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "     prompt_title                                        prompt_text  \n",
       "0  The Third Wave  Background \\r\\nThe Third Wave experiment took ...  \n",
       "1  The Third Wave  Background \\r\\nThe Third Wave experiment took ...  \n",
       "2  The Third Wave  Background \\r\\nThe Third Wave experiment took ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-shanghai",
   "metadata": {
    "papermill": {
     "duration": 0.008896,
     "end_time": "2023-09-05T00:48:49.185562",
     "exception": false,
     "start_time": "2023-09-05T00:48:49.176666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Checking the distribution, outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surprised-province",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:49:41.033478Z",
     "iopub.status.busy": "2023-09-12T06:49:41.033022Z",
     "iopub.status.idle": "2023-09-12T06:49:52.566881Z",
     "shell.execute_reply": "2023-09-12T06:49:52.565861Z",
     "shell.execute_reply.started": "2023-09-12T06:49:41.033442Z"
    },
    "papermill": {
     "duration": 33.652297,
     "end_time": "2023-09-05T00:49:22.847016",
     "exception": false,
     "start_time": "2023-09-05T00:48:49.194719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import shutil\n",
    "\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "# pyspellcheckerのインストール\n",
    "# https://pyspellchecker.readthedocs.io/en/latest/quickstart.html\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "anticipated-intro",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:49:52.569513Z",
     "iopub.status.busy": "2023-09-12T06:49:52.568778Z",
     "iopub.status.idle": "2023-09-12T06:50:26.330759Z",
     "shell.execute_reply": "2023-09-12T06:50:26.329621Z",
     "shell.execute_reply.started": "2023-09-12T06:49:52.569482Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "specific-blackjack",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:50:26.335254Z",
     "iopub.status.busy": "2023-09-12T06:50:26.334942Z",
     "iopub.status.idle": "2023-09-12T06:50:26.340127Z",
     "shell.execute_reply": "2023-09-12T06:50:26.339239Z",
     "shell.execute_reply.started": "2023-09-12T06:50:26.335227Z"
    }
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hungry-honolulu",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:50:26.344539Z",
     "iopub.status.busy": "2023-09-12T06:50:26.343349Z",
     "iopub.status.idle": "2023-09-12T06:50:26.361138Z",
     "shell.execute_reply": "2023-09-12T06:50:26.360166Z",
     "shell.execute_reply.started": "2023-09-12T06:50:26.344506Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "typical-weight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:50:26.363704Z",
     "iopub.status.busy": "2023-09-12T06:50:26.362634Z",
     "iopub.status.idle": "2023-09-12T06:50:46.498136Z",
     "shell.execute_reply": "2023-09-12T06:50:46.497035Z",
     "shell.execute_reply.started": "2023-09-12T06:50:26.363672Z"
    },
    "papermill": {
     "duration": 0.104106,
     "end_time": "2023-09-05T00:49:22.962033",
     "exception": false,
     "start_time": "2023-09-05T00:49:22.857927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk.data.path.append('/kaggle/input/nltk-dataset/stopwords')\n",
    "nltk.download('stopwords')\n",
    "difficult_words = set(stopwords.words('english'))\n",
    "\n",
    "# SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "streaming-artwork",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:50:48.698939Z",
     "iopub.status.busy": "2023-09-12T06:50:48.698241Z",
     "iopub.status.idle": "2023-09-12T06:50:50.412180Z",
     "shell.execute_reply": "2023-09-12T06:50:50.411172Z",
     "shell.execute_reply.started": "2023-09-12T06:50:48.698902Z"
    }
   },
   "outputs": [],
   "source": [
    "wordfreqfile = open('enwiki-2023-04-13.txt', 'r')\n",
    "wordfreqlist = [line.split(' ')[0] for line in wordfreqfile.readlines()]\n",
    "wordfreqlist_500 = set(wordfreqlist[:500])\n",
    "wordfreqlist_1000 = set(wordfreqlist[:1000])\n",
    "wordfreqlist_5000 = set(wordfreqlist[:5000])\n",
    "wordfreqlist_10000 = set(wordfreqlist[:10000])\n",
    "wordfreqlist_20000 = set(wordfreqlist[:20000])\n",
    "wordfreqlist_50000 = set(wordfreqlist[:50000])\n",
    "wordfreqlist_100000 = set(wordfreqlist[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "honest-north",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:51:07.190922Z",
     "iopub.status.busy": "2023-09-12T06:51:07.190546Z",
     "iopub.status.idle": "2023-09-12T06:51:07.307540Z",
     "shell.execute_reply": "2023-09-12T06:51:07.306606Z",
     "shell.execute_reply.started": "2023-09-12T06:51:07.190890Z"
    },
    "papermill": {
     "duration": 0.021989,
     "end_time": "2023-09-05T00:49:22.993912",
     "exception": false,
     "start_time": "2023-09-05T00:49:22.971923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \n",
    "    # \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # \n",
    "    num_difficult_words = sum(1 for word in words if word.lower() not in difficult_words)\n",
    "    num_unfreq_words_500 = sum(1 for word in words if word.lower() not in wordfreqlist_500)\n",
    "    num_unfreq_words_1000 = sum(1 for word in words if word.lower() not in wordfreqlist_1000)\n",
    "    \n",
    "    num_unfreq_words_5000 = sum(1 for word in words if word.lower() not in wordfreqlist_5000)\n",
    "    num_unfreq_words_10000 = sum(1 for word in words if word.lower() not in wordfreqlist_10000)\n",
    "    num_unfreq_words_20000 = sum(1 for word in words if word.lower() not in wordfreqlist_20000)\n",
    "    num_unfreq_words_50000 = sum(1 for word in words if word.lower() not in wordfreqlist_50000)\n",
    "    num_unfreq_words_100000 = sum(1 for word in words if word.lower() not in wordfreqlist_100000)\n",
    "    # \n",
    "    lexical_diversity = len(set(words)) / len(words)\n",
    "    \n",
    "    # \n",
    "    freq_dist = FreqDist(words)\n",
    "    \n",
    "    # \n",
    "    num_top_words = len([word for word, freq in freq_dist.items() if freq >= len(words) * 0.10])\n",
    "    \n",
    "    # \n",
    "    num_interrogative = text.count('?')\n",
    "    num_exclamatory = text.count('!')\n",
    "    \n",
    "    # \n",
    "    misspelled_words = spell.unknown(words)\n",
    "    \n",
    "    # \n",
    "    return {\n",
    "        'num_words': len(words),\n",
    "        'avg_sentence_length': len(words) / len(sentences),\n",
    "        'num_difficult_words': num_difficult_words,\n",
    "        'num_unfreq_words_500':num_unfreq_words_500,\n",
    "        'num_unfreq_words_1000':num_unfreq_words_1000,\n",
    "        'num_unfreq_words_5000':num_unfreq_words_5000,\n",
    "        'num_unfreq_words_10000':num_unfreq_words_10000,\n",
    "        'num_unfreq_words_20000':num_unfreq_words_20000,\n",
    "        'num_unfreq_words_50000':num_unfreq_words_50000,\n",
    "        'num_unfreq_words_100000':num_unfreq_words_100000,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'num_top_words': num_top_words,\n",
    "        'num_interrogative': num_interrogative,\n",
    "        'num_exclamatory': num_exclamatory,\n",
    "        'num_misspelled_words': len(misspelled_words)\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_features_prompt(text):\n",
    "    \n",
    "    # \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # \n",
    "    num_difficult_words = sum(1 for word in words if word.lower() not in difficult_words)\n",
    "    num_unfreq_words_500 = sum(1 for word in words if word.lower() not in wordfreqlist_500)\n",
    "    num_unfreq_words_1000 = sum(1 for word in words if word.lower() not in wordfreqlist_1000)\n",
    "    \n",
    "    num_unfreq_words_5000 = sum(1 for word in words if word.lower() not in wordfreqlist_5000)\n",
    "    num_unfreq_words_10000 = sum(1 for word in words if word.lower() not in wordfreqlist_10000)\n",
    "    num_unfreq_words_20000 = sum(1 for word in words if word.lower() not in wordfreqlist_20000)\n",
    "    num_unfreq_words_50000 = sum(1 for word in words if word.lower() not in wordfreqlist_50000)\n",
    "    num_unfreq_words_100000 = sum(1 for word in words if word.lower() not in wordfreqlist_100000)\n",
    "    # \n",
    "    lexical_diversity = len(set(words)) / len(words)\n",
    "    \n",
    "    # \n",
    "    freq_dist = FreqDist(words)\n",
    "    \n",
    "    # \n",
    "    num_top_words = len([word for word, freq in freq_dist.items() if freq >= len(words) * 0.10])\n",
    "    \n",
    "    # \n",
    "    num_interrogative = text.count('?')\n",
    "    num_exclamatory = text.count('!')\n",
    "    \n",
    "    # \n",
    "    misspelled_words = spell.unknown(words)\n",
    "    \n",
    "    # \n",
    "    return {\n",
    "        'prompt_num_words': len(words),\n",
    "        'prompt_avg_sentence_length': len(words) / len(sentences),\n",
    "        'prompt_num_difficult_words': num_difficult_words,\n",
    "        'prompt_num_unfreq_words_500':num_unfreq_words_500,\n",
    "        'prompt_num_unfreq_words_1000':num_unfreq_words_1000,\n",
    "        'prompt_num_unfreq_words_5000':num_unfreq_words_5000,\n",
    "        'prompt_num_unfreq_words_10000':num_unfreq_words_10000,\n",
    "        'prompt_num_unfreq_words_20000':num_unfreq_words_20000,\n",
    "        'prompt_num_unfreq_words_50000':num_unfreq_words_50000,\n",
    "        'prompt_num_unfreq_words_100000':num_unfreq_words_100000,\n",
    "        'prompt_lexical_diversity': lexical_diversity,\n",
    "        'prompt_num_top_words': num_top_words,\n",
    "        'prompt_num_interrogative': num_interrogative,\n",
    "        'prompt_num_exclamatory': num_exclamatory,\n",
    "        'prompt_num_misspelled_words': len(misspelled_words)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vital-stocks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:51:15.845204Z",
     "iopub.status.busy": "2023-09-12T06:51:15.844291Z",
     "iopub.status.idle": "2023-09-12T06:53:10.458332Z",
     "shell.execute_reply": "2023-09-12T06:53:10.457319Z",
     "shell.execute_reply.started": "2023-09-12T06:51:15.845161Z"
    },
    "papermill": {
     "duration": 12.33396,
     "end_time": "2023-09-05T00:49:35.337270",
     "exception": false,
     "start_time": "2023-09-05T00:49:23.003310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "df_train_features = df_train['text'].apply(extract_features).apply(pd.Series)\n",
    "df_train_features_prompt = df_train['prompt_text'].apply(extract_features_prompt).apply(pd.Series)\n",
    "df_train = pd.concat([df_train, df_train_features,df_train_features_prompt], axis=1)\n",
    "\n",
    "df_test_features = df_test['text'].apply(extract_features).apply(pd.Series)\n",
    "df_test_features_prompt = df_test['prompt_text'].apply(extract_features_prompt).apply(pd.Series)\n",
    "df_test = pd.concat([df_test, df_test_features,df_test_features_prompt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opponent-elephant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.460958Z",
     "iopub.status.busy": "2023-09-12T06:53:10.460410Z",
     "iopub.status.idle": "2023-09-12T06:53:10.479401Z",
     "shell.execute_reply": "2023-09-12T06:53:10.478423Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.460923Z"
    }
   },
   "outputs": [],
   "source": [
    "misspell_mapping = {\n",
    "    'studentdesigned': 'student designed',\n",
    "    'teacherdesigned': 'teacher designed',\n",
    "    'genericname': 'generic name',\n",
    "    'winnertakeall': 'winner take all',\n",
    "    'studentname': 'student name',\n",
    "    'driveless': 'driverless',\n",
    "    'teachername': 'teacher name',\n",
    "    'propername': 'proper name',\n",
    "    'bestlaid': 'best laid',\n",
    "    'genericschool': 'generic school',\n",
    "    'schoolname': 'school name',\n",
    "    'winnertakesall': 'winner take all',\n",
    "    'elctoral': 'electoral',\n",
    "    'eletoral': 'electoral',\n",
    "    'genericcity': 'generic city',\n",
    "    'elctors': 'electoral',\n",
    "    'venuse': 'venue',\n",
    "    'blimplike': 'blimp like',\n",
    "    'selfdriving': 'self driving',\n",
    "    'electorals': 'electoral',\n",
    "    'nearrecord': 'near record',\n",
    "    'egyptianstyle': 'egyptian style',\n",
    "    'oddnumbered': 'odd numbered',\n",
    "    'carintensive': 'car intensive',\n",
    "    'elecoral': 'electoral',\n",
    "    'oction': 'auction',\n",
    "    'electroal': 'electoral',\n",
    "    'evennumbered': 'even numbered',\n",
    "    'mesalandforms': 'mesa landforms',\n",
    "    'electoralvote': 'electoral vote',\n",
    "    'relativename': 'relative name',\n",
    "    '22euro': 'twenty two euro',\n",
    "    'ellectoral': 'electoral',\n",
    "    'thirtyplus': 'thirty plus',\n",
    "    'collegewon': 'college won',\n",
    "    'hisher': 'higher',\n",
    "    'teacherbased': 'teacher based',\n",
    "    'computeranimated': 'computer animated',\n",
    "    'canadidate': 'candidate',\n",
    "    'studentbased': 'student based',\n",
    "    'gorethanks': 'gore thanks',\n",
    "    'clouddraped': 'cloud draped',\n",
    "    'edgarsnyder': 'edgar snyder',\n",
    "    'emotionrecognition': 'emotion recognition',\n",
    "    'landfrom': 'land form',\n",
    "    'fivedays': 'five days',\n",
    "    'electoal': 'electoral',\n",
    "    'lanform': 'land form',\n",
    "    'electral': 'electoral',\n",
    "    'presidentbut': 'president but',\n",
    "    'teacherassigned': 'teacher assigned',\n",
    "    'beacuas': 'because',\n",
    "    'positionestimating': 'position estimating',\n",
    "    'selfeducation': 'self education',\n",
    "    'diverless': 'driverless',\n",
    "    'computerdriven': 'computer driven',\n",
    "    'outofcontrol': 'out of control',\n",
    "    'faultthe': 'fault the',\n",
    "    'unfairoutdated': 'unfair outdated',\n",
    "    'aviods': 'avoid',\n",
    "    'momdad': 'mom dad',\n",
    "    'statesbig': 'states big',\n",
    "    'presidentswing': 'president swing',\n",
    "    'inconclusion': 'in conclusion',\n",
    "    'handsonlearning': 'hands on learning',\n",
    "    'electroral': 'electoral',\n",
    "    'carowner': 'car owner',\n",
    "    'elecotral': 'electoral',\n",
    "    'studentassigned': 'student assigned',\n",
    "    'collegefive': 'college five',\n",
    "    'presidant': 'president',\n",
    "    'unfairoutdatedand': 'unfair outdated and',\n",
    "    'nixonjimmy': 'nixon jimmy',\n",
    "    'canadates': 'candidate',\n",
    "    'tabletennis': 'table tennis',\n",
    "    'himher': 'him her',\n",
    "    'studentsummerpacketdesigners': 'student summer packet designers',\n",
    "    'studentdesign': 'student designed',\n",
    "    'limting': 'limiting',\n",
    "    'electrol': 'electoral',\n",
    "    'campaignto': 'campaign to',\n",
    "    'presendent': 'president',\n",
    "    'thezebra': 'the zebra',\n",
    "    'landformation': 'land formation',\n",
    "    'eyetoeye': 'eye to eye',\n",
    "    'selfreliance': 'self reliance',\n",
    "    'studentdriven': 'student driven',\n",
    "    'winnertake': 'winner take',\n",
    "    'alliens': 'aliens',\n",
    "    '2000but': '2000 but',\n",
    "    'electionto': 'election to',\n",
    "    'candidatesas': 'candidates as',\n",
    "    'electers': 'electoral',\n",
    "    'winnertakes': 'winner takes',\n",
    "    'isfeet': 'is feet',\n",
    "    'incar': 'incur',\n",
    "    'wellconstructed': 'well constructed',\n",
    "    'craftsmenwomen': 'crafts men women',\n",
    "    'freelunch': 'free lunch',\n",
    "    'twothousandrevolutions': 'two thousand revolutions',\n",
    "    'ushistoryorg': 'us history org',\n",
    "    'pharohs': 'pharaohs',\n",
    "    'whitehot': 'white hot',\n",
    "    'vizers': 'visors',\n",
    "    'mrjones': 'mr jones',\n",
    "    'aminute': 'a minute',\n",
    "    'spoiledmeat': 'spoiled meat',\n",
    "    'farmersgave': 'farmers gave',\n",
    "    'spolied': 'spoiled',\n",
    "    'tradgey': 'tragedy',\n",
    "    'pyrimid': 'pyramid',\n",
    "    'pyrimad': 'pyramid',\n",
    "    'egyptiansfrom': 'egyptians from',\n",
    "    'harvestthats': 'harvest that',\n",
    "    'expierment': 'experiment',\n",
    "    'jestthat': 'jest that',\n",
    "    'twothousandrevolutionsaminute': 'two thousand revolutions a minute',\n",
    "    'expirament': 'experiment',\n",
    "    'nonspoiled': 'non spoiled',\n",
    "    'egyptains': 'egyptians',\n",
    "    'tragedys': 'tragedy',\n",
    "    'pyrmaid': 'pyramid',\n",
    "    'expirment': 'experiment',\n",
    "    'whiteit': 'grade there',\n",
    "    'gradethere': 'tragedy',\n",
    "    'goverement': 'government',\n",
    "    'godsthe': 'gods the',\n",
    "    'paraoh': 'pharaoh',\n",
    "    'classesupper': 'classes upper',\n",
    "    'pharoes': 'pharaohs',\n",
    "    'noblespriests': 'noble priests',\n",
    "    'farmersslaves': 'farmers slaves',\n",
    "    'harvestâ€”thatâ€™s': 'harvest that',\n",
    "    'tradedy': 'tragedy',\n",
    "    'paraohs': 'pharaohs',\n",
    "    'paragrapgh': 'paragraph',\n",
    "    'expieriment': 'experiment',\n",
    "    'tragdey': 'tragedy',\n",
    "    'pyramaid': 'pyramid',\n",
    "    'pyrmid': 'pyramid',\n",
    "    'prists': 'priests',\n",
    "    'pharoas': 'pharaohs',\n",
    "    'priets': 'priests',\n",
    "    'pharoph': 'pharaohs',\n",
    "    'pharaoah': 'pharaohs',\n",
    "    'pharahos': 'pharaohs',\n",
    "    'pharaohthe': 'pharaohs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fabulous-hughes",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.481470Z",
     "iopub.status.busy": "2023-09-12T06:53:10.481108Z",
     "iopub.status.idle": "2023-09-12T06:53:10.511297Z",
     "shell.execute_reply": "2023-09-12T06:53:10.510271Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.481437Z"
    }
   },
   "outputs": [],
   "source": [
    "def decontraction(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"he's\", \"he is\", phrase)\n",
    "    phrase = re.sub(r\"there's\", \"there is\", phrase)\n",
    "    phrase = re.sub(r\"We're\", \"We are\", phrase)\n",
    "    phrase = re.sub(r\"That's\", \"That is\", phrase)\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"they're\", \"they are\", phrase)\n",
    "    phrase = re.sub(r\"Can't\", \"Cannot\", phrase)\n",
    "    phrase = re.sub(r\"wasn't\", \"was not\", phrase)\n",
    "    phrase = re.sub(r\"don\\x89Ûªt\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"donãât\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"aren't\", \"are not\", phrase)\n",
    "    phrase = re.sub(r\"isn't\", \"is not\", phrase)\n",
    "    phrase = re.sub(r\"What's\", \"What is\", phrase)\n",
    "    phrase = re.sub(r\"haven't\", \"have not\", phrase)\n",
    "    phrase = re.sub(r\"hasn't\", \"has not\", phrase)\n",
    "    phrase = re.sub(r\"There's\", \"There is\", phrase)\n",
    "    phrase = re.sub(r\"He's\", \"He is\", phrase)\n",
    "    phrase = re.sub(r\"It's\", \"It is\", phrase)\n",
    "    phrase = re.sub(r\"You're\", \"You are\", phrase)\n",
    "    phrase = re.sub(r\"I'M\", \"I am\", phrase)\n",
    "    phrase = re.sub(r\"shouldn't\", \"should not\", phrase)\n",
    "    phrase = re.sub(r\"wouldn't\", \"would not\", phrase)\n",
    "    phrase = re.sub(r\"i'm\", \"I am\", phrase)\n",
    "    phrase = re.sub(r\"I\\x89Ûªm\", \"I am\", phrase)\n",
    "    phrase = re.sub(r\"I'm\", \"I am\", phrase)\n",
    "    phrase = re.sub(r\"Isn't\", \"is not\", phrase)\n",
    "    phrase = re.sub(r\"Here's\", \"Here is\", phrase)\n",
    "    phrase = re.sub(r\"you've\", \"you have\", phrase)\n",
    "    phrase = re.sub(r\"you\\x89Ûªve\", \"you have\", phrase)\n",
    "    phrase = re.sub(r\"we're\", \"we are\", phrase)\n",
    "    phrase = re.sub(r\"what's\", \"what is\", phrase)\n",
    "    phrase = re.sub(r\"couldn't\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"we've\", \"we have\", phrase)\n",
    "    phrase = re.sub(r\"it\\x89Ûªs\", \"it is\", phrase)\n",
    "    phrase = re.sub(r\"doesn\\x89Ûªt\", \"does not\", phrase)\n",
    "    phrase = re.sub(r\"It\\x89Ûªs\", \"It is\", phrase)\n",
    "    phrase = re.sub(r\"Here\\x89Ûªs\", \"Here is\", phrase)\n",
    "    phrase = re.sub(r\"who's\", \"who is\", phrase)\n",
    "    phrase = re.sub(r\"I\\x89Ûªve\", \"I have\", phrase)\n",
    "    phrase = re.sub(r\"y'all\", \"you all\", phrase)\n",
    "    phrase = re.sub(r\"can\\x89Ûªt\", \"cannot\", phrase)\n",
    "    phrase = re.sub(r\"would've\", \"would have\", phrase)\n",
    "    phrase = re.sub(r\"it'll\", \"it will\", phrase)\n",
    "    phrase = re.sub(r\"we'll\", \"we will\", phrase)\n",
    "    phrase = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", phrase)\n",
    "    phrase = re.sub(r\"We've\", \"We have\", phrase)\n",
    "    phrase = re.sub(r\"he'll\", \"he will\", phrase)\n",
    "    phrase = re.sub(r\"Y'all\", \"You all\", phrase)\n",
    "    phrase = re.sub(r\"Weren't\", \"Were not\", phrase)\n",
    "    phrase = re.sub(r\"Didn't\", \"Did not\", phrase)\n",
    "    phrase = re.sub(r\"they'll\", \"they will\", phrase)\n",
    "    phrase = re.sub(r\"they'd\", \"they would\", phrase)\n",
    "    phrase = re.sub(r\"DON'T\", \"DO NOT\", phrase)\n",
    "    phrase = re.sub(r\"That\\x89Ûªs\", \"That is\", phrase)\n",
    "    phrase = re.sub(r\"they've\", \"they have\", phrase)\n",
    "    phrase = re.sub(r\"i'd\", \"I would\", phrase)\n",
    "    phrase = re.sub(r\"should've\", \"should have\", phrase)\n",
    "    phrase = re.sub(r\"You\\x89Ûªre\", \"You are\", phrase)\n",
    "    phrase = re.sub(r\"where's\", \"where is\", phrase)\n",
    "    phrase = re.sub(r\"Don\\x89Ûªt\", \"Do not\", phrase)\n",
    "    phrase = re.sub(r\"we'd\", \"we would\", phrase)\n",
    "    phrase = re.sub(r\"i'll\", \"I will\", phrase)\n",
    "    phrase = re.sub(r\"weren't\", \"were not\", phrase)\n",
    "    phrase = re.sub(r\"They're\", \"They are\", phrase)\n",
    "    phrase = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", phrase)\n",
    "    phrase = re.sub(r\"you\\x89Ûªll\", \"you will\", phrase)\n",
    "    phrase = re.sub(r\"I\\x89Ûªd\", \"I would\", phrase)\n",
    "    phrase = re.sub(r\"let's\", \"let us\", phrase)\n",
    "    phrase = re.sub(r\"it's\", \"it is\", phrase)\n",
    "    phrase = re.sub(r\"can't\", \"cannot\", phrase)\n",
    "    phrase = re.sub(r\"don't\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"you're\", \"you are\", phrase)\n",
    "    phrase = re.sub(r\"i've\", \"I have\", phrase)\n",
    "    phrase = re.sub(r\"that's\", \"that is\", phrase)\n",
    "    phrase = re.sub(r\"i'll\", \"I will\", phrase)\n",
    "    phrase = re.sub(r\"doesn't\", \"does not\",phrase)\n",
    "    phrase = re.sub(r\"i'd\", \"I would\", phrase)\n",
    "    phrase = re.sub(r\"didn't\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"ain't\", \"am not\", phrase)\n",
    "    phrase = re.sub(r\"you'll\", \"you will\", phrase)\n",
    "    phrase = re.sub(r\"I've\", \"I have\", phrase)\n",
    "    phrase = re.sub(r\"Don't\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"I'll\", \"I will\", phrase)\n",
    "    phrase = re.sub(r\"I'd\", \"I would\", phrase)\n",
    "    phrase = re.sub(r\"Let's\", \"Let us\", phrase)\n",
    "    phrase = re.sub(r\"you'd\", \"You would\", phrase)\n",
    "    phrase = re.sub(r\"It's\", \"It is\", phrase)\n",
    "    phrase = re.sub(r\"Ain't\", \"am not\", phrase)\n",
    "    phrase = re.sub(r\"Haven't\", \"Have not\", phrase)\n",
    "    phrase = re.sub(r\"Could've\", \"Could have\", phrase)\n",
    "    phrase = re.sub(r\"youve\", \"you have\", phrase)  \n",
    "    phrase = re.sub(r\"donå«t\", \"do not\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "upset-medicaid",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.514537Z",
     "iopub.status.busy": "2023-09-12T06:53:10.514166Z",
     "iopub.status.idle": "2023-09-12T06:53:10.525285Z",
     "shell.execute_reply": "2023-09-12T06:53:10.524343Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.514505Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = decontraction(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "republican-velvet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.527109Z",
     "iopub.status.busy": "2023-09-12T06:53:10.526595Z",
     "iopub.status.idle": "2023-09-12T06:53:10.540175Z",
     "shell.execute_reply": "2023-09-12T06:53:10.539232Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.527076Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in list(string.punctuation):\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brazilian-bones",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.542001Z",
     "iopub.status.busy": "2023-09-12T06:53:10.541679Z",
     "iopub.status.idle": "2023-09-12T06:53:10.551192Z",
     "shell.execute_reply": "2023-09-12T06:53:10.550244Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.541970Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bacterial-johns",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.554543Z",
     "iopub.status.busy": "2023-09-12T06:53:10.554246Z",
     "iopub.status.idle": "2023-09-12T06:53:10.561530Z",
     "shell.execute_reply": "2023-09-12T06:53:10.560702Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.554513Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_misspell(text):\n",
    "    for bad_word in misspell_mapping:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, misspell_mapping[bad_word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "innovative-arrival",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.563354Z",
     "iopub.status.busy": "2023-09-12T06:53:10.562850Z",
     "iopub.status.idle": "2023-09-12T06:53:10.573559Z",
     "shell.execute_reply": "2023-09-12T06:53:10.572610Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.563324Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_count(sent):\n",
    "    nn_count = 0   #Noun\n",
    "    pr_count = 0   #Pronoun\n",
    "    vb_count = 0   #Verb\n",
    "    jj_count = 0   #Adjective\n",
    "    uh_count = 0   #Interjection\n",
    "    cd_count = 0   #Numerics\n",
    "    \n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "\n",
    "    for token in sent:\n",
    "        if token[1] in ['NN','NNP','NNS']:\n",
    "            nn_count += 1\n",
    "\n",
    "        if token[1] in ['PRP','PRP$']:\n",
    "            pr_count += 1\n",
    "\n",
    "        if token[1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            vb_count += 1\n",
    "\n",
    "        if token[1] in ['JJ','JJR','JJS']:\n",
    "            jj_count += 1\n",
    "\n",
    "        if token[1] in ['UH']:\n",
    "            uh_count += 1\n",
    "\n",
    "        if token[1] in ['CD']:\n",
    "            cd_count += 1\n",
    "    \n",
    "    return pd.Series([nn_count, pr_count, vb_count, jj_count, uh_count, cd_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "latest-wallace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.575504Z",
     "iopub.status.busy": "2023-09-12T06:53:10.574887Z",
     "iopub.status.idle": "2023-09-12T06:53:10.583489Z",
     "shell.execute_reply": "2023-09-12T06:53:10.582528Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.575473Z"
    }
   },
   "outputs": [],
   "source": [
    "# from autocorrect import Speller\n",
    "# from spellchecker import SpellChecker\n",
    "# speller = Speller(lang='en')\n",
    "# spellchecker = SpellChecker()\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adolescent-waters",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.588545Z",
     "iopub.status.busy": "2023-09-12T06:53:10.587638Z",
     "iopub.status.idle": "2023-09-12T06:53:10.595224Z",
     "shell.execute_reply": "2023-09-12T06:53:10.594273Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.588513Z"
    }
   },
   "outputs": [],
   "source": [
    "# def add_spelling_dictionary(tokens):\n",
    "#     spellchecker.word_frequency.load_words(tokens)\n",
    "#     speller.nlp_data.update({token:1000 for token in tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "secret-chuck",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.598875Z",
     "iopub.status.busy": "2023-09-12T06:53:10.598623Z",
     "iopub.status.idle": "2023-09-12T06:53:10.605340Z",
     "shell.execute_reply": "2023-09-12T06:53:10.604497Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.598853Z"
    }
   },
   "outputs": [],
   "source": [
    "# def spelling(text):\n",
    "#     wordlist = text.split()\n",
    "#     amount_miss = len(list(spellchecker.unknown(wordlist)))\n",
    "#     return amount_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "military-burke",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.608770Z",
     "iopub.status.busy": "2023-09-12T06:53:10.608518Z",
     "iopub.status.idle": "2023-09-12T06:53:10.620944Z",
     "shell.execute_reply": "2023-09-12T06:53:10.620057Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.608748Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_overlap_count(row):\n",
    "    def check_is_stop_word(word):\n",
    "        return word in difficult_words\n",
    "    \n",
    "    prompt_words = row['prompt_tokens']\n",
    "    summary_words = row['summary_tokens']\n",
    "    \n",
    "    if difficult_words:\n",
    "        prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "        summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "    \n",
    "    return len(set(prompt_words).intersection(set(summary_words)))\n",
    "\n",
    "def ngrams(token, n):\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def ngram_co_occurrence(row, n):\n",
    "    original_tokens = row['prompt_tokens']\n",
    "    summary_tokens = row['summary_tokens']\n",
    "\n",
    "    original_ngrams = set(ngrams(original_tokens, n))\n",
    "    summary_ngrams = set(ngrams(summary_tokens, n))\n",
    "\n",
    "    common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "    return len(common_ngrams)\n",
    "\n",
    "\n",
    "def quotes_count(row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "knowing-drink",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.624321Z",
     "iopub.status.busy": "2023-09-12T06:53:10.623253Z",
     "iopub.status.idle": "2023-09-12T06:53:10.643176Z",
     "shell.execute_reply": "2023-09-12T06:53:10.642186Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.624290Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocess(data):\n",
    "    data[\"prompt_length\"] = data[\"prompt_text\"].progress_apply(lambda x: len(word_tokenize(x)))\n",
    "    data[\"prompt_tokens\"] = data[\"prompt_text\"].progress_apply(lambda x: word_tokenize(x))\n",
    "    data[\"summary_length\"] = data[\"text\"].progress_apply(lambda x: len(word_tokenize(x)))\n",
    "    data[\"summary_tokens\"] = data[\"text\"].progress_apply(lambda x: word_tokenize(x))\n",
    "#     data[\"prompt_tokens\"].progress_apply(lambda x: add_spelling_dictionary(x))\n",
    "    #summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(lambda x: speller(x))\n",
    "#     data[\"splling_err_num\"] = data[\"text\"].progress_apply(spelling)\n",
    "    \n",
    "    df = data\n",
    "    df['length_ratio'] = df['summary_length'] / df['prompt_length']\n",
    "    df['word_overlap_count'] = df.progress_apply(word_overlap_count, axis=1)\n",
    "    df['bigram_overlap_count'] = df.progress_apply(ngram_co_occurrence, args=(2,), axis=1)\n",
    "    df['bigram_overlap_ratio'] = df['bigram_overlap_count'] / (df['summary_length'] - 1)\n",
    "    df['trigram_overlap_count'] = df.progress_apply(ngram_co_occurrence, args=(3,), axis=1)\n",
    "    df['trigram_overlap_ratio'] = df['trigram_overlap_count'] / (df['summary_length'] - 2)\n",
    "    df['quotes_count'] = df.progress_apply(quotes_count, axis=1)\n",
    "    return df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "valuable-indiana",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:53:10.649875Z",
     "iopub.status.busy": "2023-09-12T06:53:10.648127Z",
     "iopub.status.idle": "2023-09-12T06:55:32.608570Z",
     "shell.execute_reply": "2023-09-12T06:55:32.607439Z",
     "shell.execute_reply.started": "2023-09-12T06:53:10.649651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:20<00:00, 346.68it/s]\n",
      "100%|██████████| 7165/7165 [00:20<00:00, 343.33it/s]\n",
      "100%|██████████| 7165/7165 [00:02<00:00, 2960.68it/s]\n",
      "100%|██████████| 7165/7165 [00:02<00:00, 2987.08it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 11267.90it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 8401.66it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 7459.44it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 106890.99it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = text_preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "persistent-lafayette",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:55:32.610976Z",
     "iopub.status.busy": "2023-09-12T06:55:32.610468Z",
     "iopub.status.idle": "2023-09-12T06:57:46.770249Z",
     "shell.execute_reply": "2023-09-12T06:57:46.769063Z",
     "shell.execute_reply.started": "2023-09-12T06:55:32.610942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:00<00:00, 308965.92it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 143932.24it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 1196916.85it/s]\n",
      "100%|██████████| 7165/7165 [00:26<00:00, 268.61it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 5127.44it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 187105.82it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 171055.28it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 67077.93it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 942252.09it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 767557.74it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 3629.26it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 3626.03it/s]\n",
      "100%|██████████| 7165/7165 [00:15<00:00, 466.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>61</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.334848</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>203</td>\n",
       "      <td>16.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.048203</td>\n",
       "      <td>0.355229</td>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0095993991fe</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave only started as an experiment w...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>60</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c20c6ddd23</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The experimen was orginally about how even whe...</td>\n",
       "      <td>0.567975</td>\n",
       "      <td>0.969062</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>76</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d40ad10dc9</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave developed so quickly due to the...</td>\n",
       "      <td>-0.910596</td>\n",
       "      <td>-0.081769</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>27</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.088939</td>\n",
       "      <td>0.325909</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "2  0095993991fe    814d6b  The third wave only started as an experiment w...   \n",
       "3  00c20c6ddd23    814d6b  The experimen was orginally about how even whe...   \n",
       "4  00d40ad10dc9    814d6b  The third wave developed so quickly due to the...   \n",
       "\n",
       "    content   wording                                    prompt_question  \\\n",
       "0  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "1  3.272894  3.219757  Summarize how the Third Wave developed over su...   \n",
       "2  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "3  0.567975  0.969062  Summarize how the Third Wave developed over su...   \n",
       "4 -0.910596 -0.081769  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "     prompt_title                                        prompt_text  \\\n",
       "0  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "1  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "2  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "3  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "4  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "\n",
       "   num_words  avg_sentence_length  ...  num_paragraphs  num_sentences  \\\n",
       "0         61            16.000000  ...               1              4   \n",
       "1        203            16.571429  ...               1             14   \n",
       "2         60            22.333333  ...               1              6   \n",
       "3         76            28.666667  ...               1              4   \n",
       "4         27            14.500000  ...               1              3   \n",
       "\n",
       "   polarity  subjectivity  nn_count  pr_count  vb_count  jj_count  uh_count  \\\n",
       "0  0.170455      0.334848        14         3        17         6         0   \n",
       "1  0.048203      0.355229        59        11        37         7         0   \n",
       "2  0.075000      0.318750        16         4        12         3         0   \n",
       "3 -0.666667      0.666667        17         4        15         6         0   \n",
       "4  0.088939      0.325909         5         2         4         4         0   \n",
       "\n",
       "   cd_count  \n",
       "0         1  \n",
       "1         6  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"num_words\"] = df_train[\"text\"].progress_apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_chars\"] = df_train[\"text\"].progress_apply(lambda x: len(str(x)))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].progress_apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_paragraphs\"] = df_train[\"text\"].progress_apply(lambda x: len(x.split('\\n')))\n",
    "df_train[\"num_sentences\"] = df_train[\"text\"].progress_apply(lambda x: len(str(x).split('.')))\n",
    "df_train[\"polarity\"] = df_train['text'].progress_apply(lambda x: TextBlob(x).sentiment[0])\n",
    "df_train[\"subjectivity\"] = df_train['text'].progress_apply(lambda x: TextBlob(x).sentiment[1])\n",
    "df_train[['nn_count','pr_count','vb_count','jj_count','uh_count','cd_count']] = df_train['text'].progress_apply(pos_count)\n",
    "\n",
    "# df_train[\"prompt_num_words\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len(str(x).split()))\n",
    "# df_train[\"prompt_num_unique_words\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "# df_train[\"prompt_num_chars\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len(str(x)))\n",
    "# df_train[\"prompt_num_stopwords\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "# df_train[\"prompt_num_punctuations\"] =df_train['prompt_text'].progress_apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\n",
    "# df_train[\"prompt_num_words_upper\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "# df_train[\"prompt_num_words_title\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "# df_train[\"prompt_mean_word_len\"] = df_train[\"prompt_text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "# df_train[\"prompt_num_paragraphs\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len(x.split('\\n')))\n",
    "# df_train[\"prompt_num_sentences\"] = df_train[\"prompt_text\"].progress_apply(lambda x: len(str(x).split('.')))\n",
    "# df_train[\"prompt_polarity\"] = df_train['prompt_text'].progress_apply(lambda x: TextBlob(x).sentiment[0])\n",
    "# df_train[\"prompt_subjectivity\"] = df_train['prompt_text'].progress_apply(lambda x: TextBlob(x).sentiment[1])\n",
    "# df_train[['prompt_nn_count','prompt_pr_count','prompt_vb_count','prompt_jj_count','prompt_uh_count','prompt_cd_count']] = df_train['prompt_text'].progress_apply(pos_count)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "automotive-teacher",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:57:46.772592Z",
     "iopub.status.busy": "2023-09-12T06:57:46.772091Z",
     "iopub.status.idle": "2023-09-12T06:57:46.828686Z",
     "shell.execute_reply": "2023-09-12T06:57:46.827698Z",
     "shell.execute_reply.started": "2023-09-12T06:57:46.772553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 8820.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 8586.09it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7869.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7778.03it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3888.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4394.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4432.55it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4655.17it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = text_preprocess(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fancy-trademark",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:57:46.831264Z",
     "iopub.status.busy": "2023-09-12T06:57:46.830435Z",
     "iopub.status.idle": "2023-09-12T06:57:46.939746Z",
     "shell.execute_reply": "2023-09-12T06:57:46.938759Z",
     "shell.execute_reply.started": "2023-09-12T06:57:46.831227Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 24600.02it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 26886.56it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 16304.39it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2934.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 15577.73it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11169.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 18703.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 14807.78it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 18872.01it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 19152.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5090.17it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4950.49it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1467.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_difficult_words</th>\n",
       "      <th>num_unfreq_words_500</th>\n",
       "      <th>...</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text prompt_question     prompt_title  \\\n",
       "0  000000ffffff    abc123  Example text 1    Summarize...  Example Title 1   \n",
       "1  222222cccccc    abc123  Example text 3    Summarize...  Example Title 1   \n",
       "2  111111eeeeee    def789  Example text 2    Summarize...  Example Title 2   \n",
       "3  333333dddddd    def789  Example text 4    Summarize...  Example Title 2   \n",
       "\n",
       "        prompt_text  num_words  avg_sentence_length  num_difficult_words  \\\n",
       "0  Heading\\nText...          3                  3.0                  3.0   \n",
       "1  Heading\\nText...          3                  3.0                  3.0   \n",
       "2  Heading\\nText...          3                  3.0                  3.0   \n",
       "3  Heading\\nText...          3                  3.0                  3.0   \n",
       "\n",
       "   num_unfreq_words_500  ...  num_paragraphs  num_sentences  polarity  \\\n",
       "0                   2.0  ...               1              1       0.0   \n",
       "1                   2.0  ...               1              1       0.0   \n",
       "2                   2.0  ...               1              1       0.0   \n",
       "3                   2.0  ...               1              1       0.0   \n",
       "\n",
       "   subjectivity  nn_count  pr_count  vb_count  jj_count  uh_count  cd_count  \n",
       "0           0.0         2         0         0         0         0         1  \n",
       "1           0.0         2         0         0         0         0         1  \n",
       "2           0.0         2         0         0         0         0         1  \n",
       "3           0.0         2         0         0         0         0         1  \n",
       "\n",
       "[4 rows x 62 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"num_words\"] = df_test[\"text\"].progress_apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_chars\"] = df_test[\"text\"].progress_apply(lambda x: len(str(x)))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].progress_apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_paragraphs\"] = df_test[\"text\"].progress_apply(lambda x: len(x.split('\\n')))\n",
    "df_test[\"num_sentences\"] = df_test[\"text\"].progress_apply(lambda x: len(str(x).split('.')))\n",
    "df_test[\"polarity\"] = df_test['text'].progress_apply(lambda x: TextBlob(x).sentiment[0])\n",
    "df_test[\"subjectivity\"] = df_test['text'].progress_apply(lambda x: TextBlob(x).sentiment[1])\n",
    "df_test[['nn_count','pr_count','vb_count','jj_count','uh_count','cd_count']] = df_test['text'].progress_apply(pos_count)\n",
    "\n",
    "# df_test[\"prompt_num_words\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len(str(x).split()))\n",
    "# df_test[\"prompt_num_unique_words\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len(set(str(x).split())))\n",
    "# df_test[\"prompt_num_chars\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len(str(x)))\n",
    "# df_test[\"prompt_num_stopwords\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "# df_test[\"prompt_num_punctuations\"] =df_test['prompt_text'].progress_apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\n",
    "# df_test[\"prompt_num_words_upper\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "# df_test[\"prompt_num_words_title\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "# df_test[\"prompt_mean_word_len\"] = df_test[\"prompt_text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "# df_test[\"prompt_num_paragraphs\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len(x.split('\\n')))\n",
    "# df_test[\"prompt_num_sentences\"] = df_test[\"prompt_text\"].progress_apply(lambda x: len(str(x).split('.')))\n",
    "# df_test[\"prompt_polarity\"] = df_test['prompt_text'].progress_apply(lambda x: TextBlob(x).sentiment[0])\n",
    "# df_test[\"prompt_subjectivity\"] = df_test['prompt_text'].progress_apply(lambda x: TextBlob(x).sentiment[1])\n",
    "# df_test[['prompt_nn_count','prompt_pr_count','prompt_vb_count','prompt_jj_count','prompt_uh_count','prompt_cd_count']] = df_test['prompt_text'].progress_apply(pos_count)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "authentic-creature",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:57:46.941630Z",
     "iopub.status.busy": "2023-09-12T06:57:46.941266Z",
     "iopub.status.idle": "2023-09-12T06:57:49.905433Z",
     "shell.execute_reply": "2023-09-12T06:57:49.904420Z",
     "shell.execute_reply.started": "2023-09-12T06:57:46.941588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:00<00:00, 13759.62it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 180375.54it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 54614.13it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 24985.32it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['text_clean'] = df_train['text'].progress_apply(clean_text)\n",
    "df_train['text_clean'] = df_train['text_clean'].progress_apply(remove_punctuations)\n",
    "df_train['text_clean'] = df_train['text_clean'].progress_apply(clean_number)\n",
    "df_train['text_clean'] = df_train['text_clean'].progress_apply(clean_misspell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "convinced-algeria",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:57:49.907449Z",
     "iopub.status.busy": "2023-09-12T06:57:49.906820Z",
     "iopub.status.idle": "2023-09-12T06:57:49.934469Z",
     "shell.execute_reply": "2023-09-12T06:57:49.933499Z",
     "shell.execute_reply.started": "2023-09-12T06:57:49.907413Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10180.35it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 12018.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11259.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11008.67it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test['text_clean'] = df_test['text'].progress_apply(clean_text)\n",
    "df_test['text_clean'] = df_test['text_clean'].progress_apply(remove_punctuations)\n",
    "df_test['text_clean'] = df_test['text_clean'].progress_apply(clean_number)\n",
    "df_test['text_clean'] = df_test['text_clean'].progress_apply(clean_misspell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-boost",
   "metadata": {
    "papermill": {
     "duration": 0.017472,
     "end_time": "2023-09-05T00:49:38.426262",
     "exception": false,
     "start_time": "2023-09-05T00:49:38.408790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Creation\n",
    "Splitting the data into training and testing sets  \n",
    "Checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "female-hybrid",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T00:09:07.047957Z",
     "iopub.status.busy": "2023-09-11T00:09:07.047293Z",
     "iopub.status.idle": "2023-09-11T00:09:08.303914Z",
     "shell.execute_reply": "2023-09-11T00:09:08.302913Z",
     "shell.execute_reply.started": "2023-09-11T00:09:07.047921Z"
    },
    "papermill": {
     "duration": 2.840601,
     "end_time": "2023-09-05T00:49:41.284225",
     "exception": false,
     "start_time": "2023-09-05T00:49:38.443624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-planner",
   "metadata": {
    "papermill": {
     "duration": 0.017435,
     "end_time": "2023-09-05T00:50:03.955382",
     "exception": false,
     "start_time": "2023-09-05T00:50:03.937947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training\n",
    "Training the model and monitoring the progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dominant-implement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:00:23.601431Z",
     "iopub.status.busy": "2023-09-12T07:00:23.600409Z",
     "iopub.status.idle": "2023-09-12T07:00:23.607493Z",
     "shell.execute_reply": "2023-09-12T07:00:23.606164Z",
     "shell.execute_reply.started": "2023-09-12T07:00:23.601383Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name=\"deberta-v3-base\"\n",
    "    learning_rate=1.5e-5\n",
    "    weight_decay=0.02 # Regularization 防止过拟合\n",
    "    hidden_dropout_prob=0.007 # Dropout setting 随机失活的概率\n",
    "    attention_probs_dropout_prob=0.007\n",
    "    num_train_epochs= 1\n",
    "    n_splits=4\n",
    "    batch_size=4\n",
    "    random_seed=42\n",
    "    save_steps=100 \n",
    "    max_length=512 # Max length of imput 限制输入序列的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "straight-august",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:00:25.020070Z",
     "iopub.status.busy": "2023-09-12T07:00:25.019651Z",
     "iopub.status.idle": "2023-09-12T07:00:25.032187Z",
     "shell.execute_reply": "2023-09-12T07:00:25.030997Z",
     "shell.execute_reply.started": "2023-09-12T07:00:25.020037Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    return (content_score + wording_score)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "every-raleigh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:00:25.584872Z",
     "iopub.status.busy": "2023-09-12T07:00:25.584504Z",
     "iopub.status.idle": "2023-09-12T07:00:25.610743Z",
     "shell.execute_reply": "2023-09-12T07:00:25.609712Z",
     "shell.execute_reply.started": "2023-09-12T07:00:25.584842Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContentScoreRegressor: \n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                model_dir: str,\n",
    "                target: str,\n",
    "                hidden_dropout_prob: float,\n",
    "                attention_probs_dropout_prob: float,\n",
    "                max_length: int,\n",
    "                ):\n",
    "        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"text\"]#\"fixed_summary_text\"]\n",
    "        self.input_col = \"input\"\n",
    "        \n",
    "        self.text_cols = [self.input_col] \n",
    "        self.target = target\n",
    "        self.target_cols = [target]\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model_dir = model_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/deberta-v3-base\")\n",
    "        self.model_config = AutoConfig.from_pretrained(f\"microsoft/deberta-v3-base\")\n",
    "        \n",
    "        self.model_config.update({\n",
    "            \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
    "            \"num_labels\": 1,\n",
    "            \"problem_type\": \"regression\",\n",
    "        })\n",
    "        \n",
    "        seed_everything(seed=42)\n",
    "\n",
    "        self.data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "\n",
    "    def tokenize_function(self, examples: pd.DataFrame):\n",
    "        labels = [examples[self.target]]\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def tokenize_function_test(self, examples: pd.DataFrame):\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return tokenized\n",
    "        \n",
    "    def train(self, \n",
    "            fold: int,\n",
    "            train_df: pd.DataFrame,\n",
    "            valid_df: pd.DataFrame,\n",
    "            batch_size: int,\n",
    "            learning_rate: float,\n",
    "            weight_decay: float,\n",
    "            num_train_epochs: float,\n",
    "            save_steps: int,\n",
    "        ) -> None:\n",
    "        \"\"\"fine-tuning\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        train_df[self.input_col] = (\n",
    "                    train_df[\"prompt_title\"] + sep \n",
    "                    + train_df[\"prompt_question\"] + sep \n",
    "                    + train_df[\"text\"]\n",
    "                  )\n",
    "\n",
    "        valid_df[self.input_col] = (\n",
    "                    valid_df[\"prompt_title\"] + sep \n",
    "                    + valid_df[\"prompt_question\"] + sep \n",
    "                    + valid_df[\"text\"]\n",
    "                  )\n",
    "        \n",
    "        train_df = train_df[[self.input_col] + self.target_cols]\n",
    "        valid_df = valid_df[[self.input_col] + self.target_cols]\n",
    "        \n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(\n",
    "            f\"microsoft/deberta-v3-base\", \n",
    "            config=self.model_config\n",
    "        )\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n",
    "        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n",
    "    \n",
    "        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n",
    "        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n",
    "\n",
    "        # eg. \"bert/fold_0/\"\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            load_best_model_at_end=True, # select best model\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            report_to='none',\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=save_steps,\n",
    "            save_steps=save_steps,\n",
    "            metric_for_best_model=\"rmse\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_content,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tokenized_datasets,\n",
    "            eval_dataset=val_tokenized_datasets,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=self.data_collator\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        model_content.save_pretrained(self.model_dir)\n",
    "        self.tokenizer.save_pretrained(self.model_dir)\n",
    "\n",
    "        \n",
    "    def predict(self, \n",
    "                test_df: pd.DataFrame,\n",
    "                fold: int,\n",
    "               ):\n",
    "        \"\"\"predict content score\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        in_text = (\n",
    "                    test_df[\"prompt_title\"] + sep \n",
    "                    + test_df[\"prompt_question\"] + sep \n",
    "                    + test_df[\"text\"]\n",
    "                  )\n",
    "        test_df[self.input_col] = in_text\n",
    "\n",
    "        test_ = test_df[[self.input_col]]\n",
    "    \n",
    "        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n",
    "        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n",
    "\n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n",
    "        model_content.eval()\n",
    "        \n",
    "        # e.g. \"bert/fold_0/\"\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "\n",
    "        test_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            do_train = False,\n",
    "            do_predict = True,\n",
    "            per_device_eval_batch_size = 4,   \n",
    "            dataloader_drop_last = False,\n",
    "        )\n",
    "\n",
    "        # init trainer\n",
    "        infer_content = Trainer(\n",
    "                      model = model_content, \n",
    "                      tokenizer=self.tokenizer,\n",
    "                      data_collator=self.data_collator,\n",
    "                      args = test_args)\n",
    "\n",
    "        preds = infer_content.predict(test_tokenized_dataset)[0]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "subtle-testament",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:00:28.737082Z",
     "iopub.status.busy": "2023-09-12T07:00:28.736717Z",
     "iopub.status.idle": "2023-09-12T07:00:28.753279Z",
     "shell.execute_reply": "2023-09-12T07:00:28.752129Z",
     "shell.execute_reply.started": "2023-09-12T07:00:28.737051Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_by_fold(\n",
    "        train_df: pd.DataFrame,\n",
    "        model_name: str,\n",
    "        target:str,\n",
    "        save_each_model: bool,\n",
    "        n_splits: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: int,\n",
    "        hidden_dropout_prob: float,\n",
    "        attention_probs_dropout_prob: float,\n",
    "        weight_decay: float,\n",
    "        num_train_epochs: int,\n",
    "        save_steps: int,\n",
    "        max_length:int\n",
    "    ):\n",
    "\n",
    "    # delete old model files\n",
    "    if os.path.exists(model_name):\n",
    "        shutil.rmtree(model_name)\n",
    "    \n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        train_data = train_df[train_df[\"fold\"] != fold]\n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        csr.train(\n",
    "            fold=fold,\n",
    "            train_df=train_data,\n",
    "            valid_df=valid_data, \n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=save_steps,\n",
    "        )\n",
    "\n",
    "def validate(\n",
    "    train_df: pd.DataFrame,\n",
    "    target:str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length : int\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"predict oof data\"\"\"\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "        \n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=valid_data, \n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n",
    "\n",
    "    return train_df\n",
    "    \n",
    "def predict(\n",
    "    test_df: pd.DataFrame,\n",
    "    target:str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length : int\n",
    "    ):\n",
    "    \"\"\"predict using mean folds\"\"\"\n",
    "\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=test_df, \n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        test_df[f\"{target}_pred_{fold}\"] = pred\n",
    "    \n",
    "    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "individual-parameter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:00:30.679707Z",
     "iopub.status.busy": "2023-09-12T07:00:30.679302Z",
     "iopub.status.idle": "2023-09-12T07:00:30.719045Z",
     "shell.execute_reply": "2023-09-12T07:00:30.717982Z",
     "shell.execute_reply.started": "2023-09-12T07:00:30.679676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>61</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.334848</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the third wave was an experimentto see how peo...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>203</td>\n",
       "      <td>16.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048203</td>\n",
       "      <td>0.355229</td>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>the third wave developed  rapidly because the ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0095993991fe</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave only started as an experiment w...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>60</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the third wave only started as an experiment w...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c20c6ddd23</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The experimen was orginally about how even whe...</td>\n",
       "      <td>0.567975</td>\n",
       "      <td>0.969062</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>76</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the experimen was orginally about how even whe...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d40ad10dc9</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave developed so quickly due to the...</td>\n",
       "      <td>-0.910596</td>\n",
       "      <td>-0.081769</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>27</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088939</td>\n",
       "      <td>0.325909</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the third wave developed so quickly due to the...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "2  0095993991fe    814d6b  The third wave only started as an experiment w...   \n",
       "3  00c20c6ddd23    814d6b  The experimen was orginally about how even whe...   \n",
       "4  00d40ad10dc9    814d6b  The third wave developed so quickly due to the...   \n",
       "\n",
       "    content   wording                                    prompt_question  \\\n",
       "0  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "1  3.272894  3.219757  Summarize how the Third Wave developed over su...   \n",
       "2  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "3  0.567975  0.969062  Summarize how the Third Wave developed over su...   \n",
       "4 -0.910596 -0.081769  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "     prompt_title                                        prompt_text  \\\n",
       "0  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "1  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "2  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "3  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "4  The Third Wave  Background \\r\\nThe Third Wave experiment took ...   \n",
       "\n",
       "   num_words  avg_sentence_length  ...  polarity  subjectivity  nn_count  \\\n",
       "0         61            16.000000  ...  0.170455      0.334848        14   \n",
       "1        203            16.571429  ...  0.048203      0.355229        59   \n",
       "2         60            22.333333  ...  0.075000      0.318750        16   \n",
       "3         76            28.666667  ... -0.666667      0.666667        17   \n",
       "4         27            14.500000  ...  0.088939      0.325909         5   \n",
       "\n",
       "   pr_count  vb_count  jj_count  uh_count  cd_count  \\\n",
       "0         3        17         6         0         1   \n",
       "1        11        37         7         0         6   \n",
       "2         4        12         3         0         0   \n",
       "3         4        15         6         0         0   \n",
       "4         2         4         4         0         0   \n",
       "\n",
       "                                          text_clean  fold  \n",
       "0  the third wave was an experimentto see how peo...   3.0  \n",
       "1  the third wave developed  rapidly because the ...   3.0  \n",
       "2  the third wave only started as an experiment w...   3.0  \n",
       "3  the experimen was orginally about how even whe...   3.0  \n",
       "4  the third wave developed so quickly due to the...   3.0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df_train.copy()\n",
    "gkf = GroupKFold(n_splits=CFG.n_splits)\n",
    "\n",
    "for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n",
    "    train.loc[val_index, \"fold\"] = i\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "noted-exhaust",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T00:09:08.443822Z",
     "iopub.status.busy": "2023-09-11T00:09:08.443394Z",
     "iopub.status.idle": "2023-09-11T00:56:54.439519Z",
     "shell.execute_reply": "2023-09-11T00:56:54.438493Z",
     "shell.execute_reply.started": "2023-09-11T00:09:08.443789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1277' max='1277' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1277/1277 04:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401398</td>\n",
       "      <td>0.633559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>0.564806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426074</td>\n",
       "      <td>0.652743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239170</td>\n",
       "      <td>0.489050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.464791</td>\n",
       "      <td>0.681756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.317144</td>\n",
       "      <td>0.563156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.214987</td>\n",
       "      <td>0.463667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.236764</td>\n",
       "      <td>0.486584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.329540</td>\n",
       "      <td>0.574055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.223200</td>\n",
       "      <td>0.270699</td>\n",
       "      <td>0.520287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.223200</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.603125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.223200</td>\n",
       "      <td>0.322709</td>\n",
       "      <td>0.568075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1289' max='1289' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1289/1289 04:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.385737</td>\n",
       "      <td>0.621077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580042</td>\n",
       "      <td>0.761605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319427</td>\n",
       "      <td>0.565179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.349180</td>\n",
       "      <td>0.590914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.293957</td>\n",
       "      <td>0.542178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.319976</td>\n",
       "      <td>0.565664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.456921</td>\n",
       "      <td>0.675959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.363039</td>\n",
       "      <td>0.602527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.336576</td>\n",
       "      <td>0.580152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.284807</td>\n",
       "      <td>0.533673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.277850</td>\n",
       "      <td>0.527115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.277117</td>\n",
       "      <td>0.526419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1293' max='1293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1293/1293 04:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>0.576037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>0.493940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.323284</td>\n",
       "      <td>0.568581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228092</td>\n",
       "      <td>0.477590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.221806</td>\n",
       "      <td>0.470963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.460005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.245315</td>\n",
       "      <td>0.495293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.229130</td>\n",
       "      <td>0.478676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.244342</td>\n",
       "      <td>0.494310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.263096</td>\n",
       "      <td>0.512929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.248733</td>\n",
       "      <td>0.498731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.255164</td>\n",
       "      <td>0.505137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1516' max='1516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1516/1516 04:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419256</td>\n",
       "      <td>0.647499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398849</td>\n",
       "      <td>0.631545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.547738</td>\n",
       "      <td>0.740093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.538486</td>\n",
       "      <td>0.733816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.471162</td>\n",
       "      <td>0.686412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.557215</td>\n",
       "      <td>0.746468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.351471</td>\n",
       "      <td>0.592850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.387574</td>\n",
       "      <td>0.622554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.343305</td>\n",
       "      <td>0.585923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.383445</td>\n",
       "      <td>0.619230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.450025</td>\n",
       "      <td>0.670839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.563935</td>\n",
       "      <td>0.750956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.483688</td>\n",
       "      <td>0.695477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.417594</td>\n",
       "      <td>0.646215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.466791</td>\n",
       "      <td>0.683221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv content rmse: 0.501216233911963\n",
      "fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1277' max='1277' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1277/1277 04:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495753</td>\n",
       "      <td>0.704097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492575</td>\n",
       "      <td>0.701837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.374369</td>\n",
       "      <td>0.611857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.342094</td>\n",
       "      <td>0.584888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.315342</td>\n",
       "      <td>0.561553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.386581</td>\n",
       "      <td>0.621757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.326614</td>\n",
       "      <td>0.571502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.346566</td>\n",
       "      <td>0.588698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.293005</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.344916</td>\n",
       "      <td>0.587295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.297711</td>\n",
       "      <td>0.545629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.291735</td>\n",
       "      <td>0.540125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1289' max='1289' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1289/1289 04:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645586</td>\n",
       "      <td>0.803484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.603143</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818556</td>\n",
       "      <td>0.904741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.813777</td>\n",
       "      <td>0.902096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.889608</td>\n",
       "      <td>0.943190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.757990</td>\n",
       "      <td>0.870626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.921968</td>\n",
       "      <td>0.960191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.703230</td>\n",
       "      <td>0.838588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.669797</td>\n",
       "      <td>0.818411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.810247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.680864</td>\n",
       "      <td>0.825145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.812636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1293' max='1293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1293/1293 04:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.771314</td>\n",
       "      <td>0.878245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.462933</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.417501</td>\n",
       "      <td>0.646143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446097</td>\n",
       "      <td>0.667905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.422573</td>\n",
       "      <td>0.650056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.313759</td>\n",
       "      <td>0.560142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.368970</td>\n",
       "      <td>0.607429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.442364</td>\n",
       "      <td>0.665104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.378227</td>\n",
       "      <td>0.615001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.326535</td>\n",
       "      <td>0.571433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.314094</td>\n",
       "      <td>0.560441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.311163</td>\n",
       "      <td>0.557820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[self.input_col] = (\n",
      "<ipython-input-41-dd746eed2117>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[self.input_col] = (\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1516' max='1516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1516/1516 04:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.127853</td>\n",
       "      <td>1.062004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868088</td>\n",
       "      <td>0.931713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.371032</td>\n",
       "      <td>1.170911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685915</td>\n",
       "      <td>0.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.508128</td>\n",
       "      <td>0.712831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.618110</td>\n",
       "      <td>0.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.519488</td>\n",
       "      <td>0.720755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.470154</td>\n",
       "      <td>0.685678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.505365</td>\n",
       "      <td>0.710890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.517265</td>\n",
       "      <td>0.719212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.447010</td>\n",
       "      <td>0.668588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.505846</td>\n",
       "      <td>0.711228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.475185</td>\n",
       "      <td>0.689337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.428051</td>\n",
       "      <td>0.654256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.452001</td>\n",
       "      <td>0.672311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<ipython-input-41-dd746eed2117>:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[self.input_col] = in_text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv wording rmse: 0.6367536772260763\n",
      "fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/yi/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in [\"content\", \"wording\"]:\n",
    "    train_by_fold(\n",
    "        train,\n",
    "        model_name=CFG.model_name,\n",
    "        save_each_model=False,\n",
    "        target=target,\n",
    "        learning_rate=CFG.learning_rate,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        num_train_epochs=CFG.num_train_epochs,\n",
    "        n_splits=CFG.n_splits,\n",
    "        batch_size=CFG.batch_size,\n",
    "        save_steps=CFG.save_steps,\n",
    "        max_length=CFG.max_length\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train = validate(\n",
    "        train,\n",
    "        target=target,\n",
    "        save_each_model=False,\n",
    "        model_name=CFG.model_name,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        max_length=CFG.max_length\n",
    "    )\n",
    "\n",
    "    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n",
    "    print(f\"cv {target} rmse: {rmse}\")\n",
    "\n",
    "    test = predict(\n",
    "        df_test,\n",
    "        target=target,\n",
    "        save_each_model=False,\n",
    "        model_name=CFG.model_name,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        max_length=CFG.max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "conscious-endorsement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T00:56:54.445568Z",
     "iopub.status.busy": "2023-09-11T00:56:54.445264Z",
     "iopub.status.idle": "2023-09-11T00:56:55.270672Z",
     "shell.execute_reply": "2023-09-11T00:56:55.269334Z",
     "shell.execute_reply.started": "2023-09-11T00:56:54.445542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7226"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-details",
   "metadata": {},
   "source": [
    "## Starting of tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fourth-briefing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_words', 'avg_sentence_length', 'num_difficult_words',\n",
       "       'num_unfreq_words_500', 'num_unfreq_words_1000',\n",
       "       'num_unfreq_words_5000', 'num_unfreq_words_10000',\n",
       "       'num_unfreq_words_20000', 'num_unfreq_words_50000',\n",
       "       'num_unfreq_words_100000', 'lexical_diversity', 'num_top_words',\n",
       "       'num_interrogative', 'num_exclamatory', 'num_misspelled_words',\n",
       "       'prompt_num_words', 'prompt_avg_sentence_length',\n",
       "       'prompt_num_difficult_words', 'prompt_num_unfreq_words_500',\n",
       "       'prompt_num_unfreq_words_1000', 'prompt_num_unfreq_words_5000',\n",
       "       'prompt_num_unfreq_words_10000', 'prompt_num_unfreq_words_20000',\n",
       "       'prompt_num_unfreq_words_50000', 'prompt_num_unfreq_words_100000',\n",
       "       'prompt_lexical_diversity', 'prompt_num_top_words',\n",
       "       'prompt_num_interrogative', 'prompt_num_exclamatory',\n",
       "       'prompt_num_misspelled_words', 'prompt_length', 'summary_length',\n",
       "       'length_ratio', 'word_overlap_count', 'bigram_overlap_count',\n",
       "       'bigram_overlap_ratio', 'trigram_overlap_count',\n",
       "       'trigram_overlap_ratio', 'quotes_count', 'num_unique_words',\n",
       "       'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper',\n",
       "       'num_words_title', 'mean_word_len', 'num_paragraphs', 'num_sentences',\n",
       "       'polarity', 'subjectivity', 'nn_count', 'pr_count', 'vb_count',\n",
       "       'jj_count', 'uh_count', 'cd_count', 'content_pred', 'wording_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "excess-oracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_words', 'avg_sentence_length', 'num_difficult_words',\n",
       "       'num_unfreq_words_500', 'num_unfreq_words_1000',\n",
       "       'num_unfreq_words_5000', 'num_unfreq_words_10000',\n",
       "       'num_unfreq_words_20000', 'num_unfreq_words_50000',\n",
       "       'num_unfreq_words_100000', 'lexical_diversity', 'num_top_words',\n",
       "       'num_interrogative', 'num_exclamatory', 'num_misspelled_words',\n",
       "       'prompt_num_words', 'prompt_avg_sentence_length',\n",
       "       'prompt_num_difficult_words', 'prompt_num_unfreq_words_500',\n",
       "       'prompt_num_unfreq_words_1000', 'prompt_num_unfreq_words_5000',\n",
       "       'prompt_num_unfreq_words_10000', 'prompt_num_unfreq_words_20000',\n",
       "       'prompt_num_unfreq_words_50000', 'prompt_num_unfreq_words_100000',\n",
       "       'prompt_lexical_diversity', 'prompt_num_top_words',\n",
       "       'prompt_num_interrogative', 'prompt_num_exclamatory',\n",
       "       'prompt_num_misspelled_words', 'prompt_length', 'summary_length',\n",
       "       'length_ratio', 'word_overlap_count', 'bigram_overlap_count',\n",
       "       'bigram_overlap_ratio', 'trigram_overlap_count',\n",
       "       'trigram_overlap_ratio', 'quotes_count', 'num_unique_words',\n",
       "       'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper',\n",
       "       'num_words_title', 'mean_word_len', 'num_paragraphs', 'num_sentences',\n",
       "       'polarity', 'subjectivity', 'nn_count', 'pr_count', 'vb_count',\n",
       "       'jj_count', 'uh_count', 'cd_count', 'content', 'wording', 'content',\n",
       "       'wording'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_copy.drop(columns=drop_columns).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "superb-broad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_difficult_words</th>\n",
       "      <th>num_unfreq_words_500</th>\n",
       "      <th>...</th>\n",
       "      <th>content_pred_0</th>\n",
       "      <th>content_pred_1</th>\n",
       "      <th>content_pred_2</th>\n",
       "      <th>content_pred_3</th>\n",
       "      <th>content</th>\n",
       "      <th>wording_pred_0</th>\n",
       "      <th>wording_pred_1</th>\n",
       "      <th>wording_pred_2</th>\n",
       "      <th>wording_pred_3</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.373640</td>\n",
       "      <td>-1.669059</td>\n",
       "      <td>-1.440567</td>\n",
       "      <td>-1.526402</td>\n",
       "      <td>-1.502417</td>\n",
       "      <td>-1.402608</td>\n",
       "      <td>-1.129992</td>\n",
       "      <td>-1.285518</td>\n",
       "      <td>-1.358818</td>\n",
       "      <td>-1.294234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.376333</td>\n",
       "      <td>-1.676393</td>\n",
       "      <td>-1.441463</td>\n",
       "      <td>-1.535597</td>\n",
       "      <td>-1.507447</td>\n",
       "      <td>-1.420122</td>\n",
       "      <td>-1.147928</td>\n",
       "      <td>-1.289689</td>\n",
       "      <td>-1.372256</td>\n",
       "      <td>-1.307498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.372934</td>\n",
       "      <td>-1.668015</td>\n",
       "      <td>-1.445778</td>\n",
       "      <td>-1.526391</td>\n",
       "      <td>-1.503279</td>\n",
       "      <td>-1.405735</td>\n",
       "      <td>-1.130658</td>\n",
       "      <td>-1.289774</td>\n",
       "      <td>-1.358575</td>\n",
       "      <td>-1.296185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.381322</td>\n",
       "      <td>-1.685353</td>\n",
       "      <td>-1.449115</td>\n",
       "      <td>-1.543126</td>\n",
       "      <td>-1.514729</td>\n",
       "      <td>-1.429919</td>\n",
       "      <td>-1.144660</td>\n",
       "      <td>-1.300050</td>\n",
       "      <td>-1.379801</td>\n",
       "      <td>-1.313608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text prompt_question     prompt_title  \\\n",
       "0  000000ffffff    abc123  Example text 1    Summarize...  Example Title 1   \n",
       "1  222222cccccc    abc123  Example text 3    Summarize...  Example Title 1   \n",
       "2  111111eeeeee    def789  Example text 2    Summarize...  Example Title 2   \n",
       "3  333333dddddd    def789  Example text 4    Summarize...  Example Title 2   \n",
       "\n",
       "        prompt_text  num_words  avg_sentence_length  num_difficult_words  \\\n",
       "0  Heading\\nText...          3                  3.0                  3.0   \n",
       "1  Heading\\nText...          3                  3.0                  3.0   \n",
       "2  Heading\\nText...          3                  3.0                  3.0   \n",
       "3  Heading\\nText...          3                  3.0                  3.0   \n",
       "\n",
       "   num_unfreq_words_500  ...  content_pred_0  content_pred_1  content_pred_2  \\\n",
       "0                   2.0  ...       -1.373640       -1.669059       -1.440567   \n",
       "1                   2.0  ...       -1.376333       -1.676393       -1.441463   \n",
       "2                   2.0  ...       -1.372934       -1.668015       -1.445778   \n",
       "3                   2.0  ...       -1.381322       -1.685353       -1.449115   \n",
       "\n",
       "   content_pred_3   content  wording_pred_0  wording_pred_1  wording_pred_2  \\\n",
       "0       -1.526402 -1.502417       -1.402608       -1.129992       -1.285518   \n",
       "1       -1.535597 -1.507447       -1.420122       -1.147928       -1.289689   \n",
       "2       -1.526391 -1.503279       -1.405735       -1.130658       -1.289774   \n",
       "3       -1.543126 -1.514729       -1.429919       -1.144660       -1.300050   \n",
       "\n",
       "   wording_pred_3   wording  \n",
       "0       -1.358818 -1.294234  \n",
       "1       -1.372256 -1.307498  \n",
       "2       -1.358575 -1.296185  \n",
       "3       -1.379801 -1.313608  \n",
       "\n",
       "[4 rows x 74 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "timely-cigarette",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:03:24.356154Z",
     "iopub.status.busy": "2023-09-12T07:03:24.355791Z",
     "iopub.status.idle": "2023-09-12T07:03:24.361501Z",
     "shell.execute_reply": "2023-09-12T07:03:24.360446Z",
     "shell.execute_reply.started": "2023-09-12T07:03:24.356125Z"
    }
   },
   "outputs": [],
   "source": [
    "train_copy = pd.concat([df_train,train[['fold', 'content_pred', 'wording_pred']]],axis = 1)\n",
    "test_copy = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "protected-perth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:01:57.991162Z",
     "iopub.status.busy": "2023-09-12T07:01:57.990798Z",
     "iopub.status.idle": "2023-09-12T07:01:57.996739Z",
     "shell.execute_reply": "2023-09-12T07:01:57.995668Z",
     "shell.execute_reply.started": "2023-09-12T07:01:57.991133Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop_columns = [\n",
    "#                 #\"fold\", \n",
    "#                 \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n",
    "#                 \"prompt_question\", \"prompt_title\", \n",
    "#                 \"prompt_text\",\n",
    "#                 \"input\"\n",
    "#                ] + [\n",
    "#                 f\"content_pred_{i}\" for i in range(CFG.n_splits)\n",
    "#                 ] + [\n",
    "#                 f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n",
    "#                 ]\n",
    "targets = [\"content\", \"wording\"]\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \n",
    "                \"prompt_question\", \"prompt_title\", 'text_clean',\n",
    "                \"prompt_text\"\n",
    "               ] + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "documented-spread",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:01:59.714845Z",
     "iopub.status.busy": "2023-09-12T07:01:59.713876Z",
     "iopub.status.idle": "2023-09-12T07:01:59.720756Z",
     "shell.execute_reply": "2023-09-12T07:01:59.719706Z",
     "shell.execute_reply.started": "2023-09-12T07:01:59.714799Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "novel-share",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:02:00.179670Z",
     "iopub.status.busy": "2023-09-12T07:02:00.178870Z",
     "iopub.status.idle": "2023-09-12T07:02:04.353645Z",
     "shell.execute_reply": "2023-09-12T07:02:04.352656Z",
     "shell.execute_reply.started": "2023-09-12T07:02:00.179627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5307\n",
      "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.017606\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.395511\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttrain's rmse: 0.394583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5082\n",
      "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score -0.039959\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.505873\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttrain's rmse: 0.505873\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5183\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.013356\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.424143\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttrain's rmse: 0.422928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5345\n",
      "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.044904\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttrain's rmse: 0.539725\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5307\n",
      "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score -0.031791\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.505725\n",
      "Early stopping, best iteration is:\n",
      "[86]\ttrain's rmse: 0.50517\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5082\n",
      "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score -0.060941\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.650368\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttrain's rmse: 0.649116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5183\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.028040\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.50005\n",
      "Early stopping, best iteration is:\n",
      "[79]\ttrain's rmse: 0.497344\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5345\n",
      "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.168933\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.606906\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttrain's rmse: 0.605333\n"
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(CFG.n_splits):\n",
    "\n",
    "        X_train_cv = train_copy[train_copy[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train_copy[train_copy[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train_copy[train_copy[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train_copy[train_copy[\"fold\"] == fold][target]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.05,\n",
    "                  }\n",
    "\n",
    "        evaluation_results = {}\n",
    "        model = lgb.train(params,\n",
    "                          num_boost_round=10000,\n",
    "                            #categorical_feature = categorical_features,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          train_set=dtrain,\n",
    "                          valid_sets=dval,\n",
    "                          callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                               lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results)\n",
    "                            ],\n",
    "                          )\n",
    "        models.append(model)\n",
    "    \n",
    "    model_dict[target] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "exciting-transmission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:02:04.356057Z",
     "iopub.status.busy": "2023-09-12T07:02:04.355386Z",
     "iopub.status.idle": "2023-09-12T07:02:04.466426Z",
     "shell.execute_reply": "2023-09-12T07:02:04.465409Z",
     "shell.execute_reply.started": "2023-09-12T07:02:04.356021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_rmse : 0.45948373619657434\n",
      "wording_rmse : 0.5627812349099749\n",
      "mcrmse : 0.5111324855532746\n"
     ]
    }
   ],
   "source": [
    "# cv\n",
    "rmses = []\n",
    "\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = train_copy[train_copy[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train_copy[train_copy[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-exercise",
   "metadata": {
    "papermill": {
     "duration": 0.047618,
     "end_time": "2023-09-05T01:15:54.669617",
     "exception": false,
     "start_time": "2023-09-05T01:15:54.621999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction\n",
    "Making predictions on the test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "excess-consortium",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:02:55.651285Z",
     "iopub.status.busy": "2023-09-12T07:02:55.650846Z",
     "iopub.status.idle": "2023-09-12T07:02:55.657696Z",
     "shell.execute_reply": "2023-09-12T07:02:55.656801Z",
     "shell.execute_reply.started": "2023-09-12T07:02:55.651248Z"
    },
    "papermill": {
     "duration": 0.134599,
     "end_time": "2023-09-05T01:15:54.976134",
     "exception": false,
     "start_time": "2023-09-05T01:15:54.841535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "                #\"fold\", \n",
    "                \"student_id\", \"prompt_id\", \"text\", \n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\",'text_clean',\n",
    "                \"input\"\n",
    "               ] + [\n",
    "                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n",
    "                ] + [\n",
    "                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "russian-translation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:02:56.262155Z",
     "iopub.status.busy": "2023-09-12T07:02:56.261238Z",
     "iopub.status.idle": "2023-09-12T07:02:56.291092Z",
     "shell.execute_reply": "2023-09-12T07:02:56.290162Z",
     "shell.execute_reply.started": "2023-09-12T07:02:56.262095Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "    preds = []\n",
    "\n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = test_copy.drop(columns=drop_columns)\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    pred_dict[target] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fleet-specification",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:03:28.058487Z",
     "iopub.status.busy": "2023-09-12T07:03:28.057994Z",
     "iopub.status.idle": "2023-09-12T07:03:28.077726Z",
     "shell.execute_reply": "2023-09-12T07:03:28.076520Z",
     "shell.execute_reply.started": "2023-09-12T07:03:28.058447Z"
    }
   },
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    preds = pred_dict[target]\n",
    "    for i, pred in enumerate(preds):\n",
    "        test[f\"{target}_pred_{i}\"] = pred\n",
    "\n",
    "    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-thermal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T11:10:12.367197Z",
     "iopub.status.busy": "2023-08-27T11:10:12.366752Z",
     "iopub.status.idle": "2023-08-27T11:10:12.378204Z",
     "shell.execute_reply": "2023-08-27T11:10:12.37654Z",
     "shell.execute_reply.started": "2023-08-27T11:10:12.367161Z"
    },
    "papermill": {
     "duration": 0.033739,
     "end_time": "2023-09-05T01:15:55.082742",
     "exception": false,
     "start_time": "2023-09-05T01:15:55.049003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating the Submission File\n",
    "reating the file for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fleet-wells",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T07:03:30.589425Z",
     "iopub.status.busy": "2023-09-12T07:03:30.589022Z",
     "iopub.status.idle": "2023-09-12T07:03:30.599132Z",
     "shell.execute_reply": "2023-09-12T07:03:30.598036Z",
     "shell.execute_reply.started": "2023-09-12T07:03:30.589386Z"
    },
    "papermill": {
     "duration": 0.026731,
     "end_time": "2023-09-05T01:15:55.132993",
     "exception": false,
     "start_time": "2023-09-05T01:15:55.106262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1640.419399,
   "end_time": "2023-09-05T01:15:59.035365",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-05T00:48:38.615966",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
